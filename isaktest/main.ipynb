{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=18432, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=8, bias=True)\n",
      "  (drop1): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from model import Net\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv('../data/AFLW/annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16418, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>face_id</th>\n",
       "      <th>LeftEye_x</th>\n",
       "      <th>LeftEye_y</th>\n",
       "      <th>RightEye_x</th>\n",
       "      <th>RightEye_y</th>\n",
       "      <th>Nose_x</th>\n",
       "      <th>Nose_y</th>\n",
       "      <th>Mouth_x</th>\n",
       "      <th>Mouth_y</th>\n",
       "      <th>file_id</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39341</td>\n",
       "      <td>209.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>277.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>image00035.jpg</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>39343</td>\n",
       "      <td>148.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>219.053909</td>\n",
       "      <td>272.376251</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>image00168.jpg</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>39348</td>\n",
       "      <td>143.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>159.983505</td>\n",
       "      <td>209.756149</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>image00102.jpg</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>39349</td>\n",
       "      <td>229.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>image00104.jpg</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>39352</td>\n",
       "      <td>334.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>486.200958</td>\n",
       "      <td>406.574982</td>\n",
       "      <td>433.417664</td>\n",
       "      <td>518.424988</td>\n",
       "      <td>image00122.jpg</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  face_id  LeftEye_x  LeftEye_y  RightEye_x  RightEye_y      Nose_x  \\\n",
       "0      0    39341      209.0      150.0       302.0       186.0  277.000000   \n",
       "1      1    39343      148.0      203.0       234.0       207.0  219.053909   \n",
       "2      2    39348      143.0      123.0       223.0       173.0  159.983505   \n",
       "3      3    39349      229.0      196.0       291.0       198.0  274.000000   \n",
       "4      4    39352      334.0      229.0       526.0       249.0  486.200958   \n",
       "\n",
       "       Nose_y     Mouth_x     Mouth_y         file_id sex  \n",
       "0  268.000000  232.000000  308.000000  image00035.jpg   m  \n",
       "1  272.376251  203.000000  308.000000  image00168.jpg   f  \n",
       "2  209.756149  134.000000  257.000000  image00102.jpg   f  \n",
       "3  218.000000  270.000000  254.000000  image00104.jpg   f  \n",
       "4  406.574982  433.417664  518.424988  image00122.jpg   f  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.rename(columns={\"Unnamed: 0\": \"index\"}, inplace=True)\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "dataset_path = os.getenv(\"DATA_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class FaceKeypointDataset(Dataset):\n",
    "    def __init__(self, annotations, root_dir, transform=None):\n",
    "        \n",
    "        self.annotations = annotations\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.annotations[\"file_id\"][idx])\n",
    "        image = plt.imread(img_name)\n",
    "        \n",
    "        if len(image.shape) == 2:\n",
    "            image = np.stack([image] * 3, axis=-1)\n",
    "\n",
    "        landmarks = self.annotations.iloc[idx, 2:-2].values\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        h, w = image.shape[1:]\n",
    "        landmarks[:, 0] *= (224 / w)\n",
    "        landmarks[:, 1] *= (224 / h)\n",
    "\n",
    "        return image, torch.tensor(landmarks, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([np.float64(209.0), np.float64(150.0), np.float64(302.0),\n",
       "       np.float64(186.0), np.float64(277.0), np.float64(268.0),\n",
       "       np.float64(232.0), np.float64(308.0)], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.iloc[0, 2:10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 636.0000,  300.0000],\n",
       "         [ 729.0000,  286.0000],\n",
       "         [ 692.0000,  353.0000],\n",
       "         [ 709.0000,  408.0000]],\n",
       "\n",
       "        [[ 996.0000, 1380.0000],\n",
       "         [1483.0000, 1309.0000],\n",
       "         [1203.0000, 1718.0000],\n",
       "         [1318.0000, 1830.0000]],\n",
       "\n",
       "        [[ 337.0000,  142.0000],\n",
       "         [ 374.0000,  139.0000],\n",
       "         [ 358.0000,  158.0000],\n",
       "         [ 359.0000,  178.0000]],\n",
       "\n",
       "        [[ 211.0000,  174.0000],\n",
       "         [ 256.0000,  179.0000],\n",
       "         [ 226.0000,  198.0000],\n",
       "         [ 227.0000,  215.0000]],\n",
       "\n",
       "        [[ 371.0000,  115.0000],\n",
       "         [ 415.0000,  117.0000],\n",
       "         [ 393.0000,  135.0000],\n",
       "         [ 393.0000,  158.0000]],\n",
       "\n",
       "        [[1930.0000, 2024.0000],\n",
       "         [2176.0000, 1700.0000],\n",
       "         [2286.0000, 1969.0000],\n",
       "         [2413.0000, 2111.0000]],\n",
       "\n",
       "        [[ 742.0000,  262.0000],\n",
       "         [ 846.0000,  298.0000],\n",
       "         [ 850.0000,  358.0000],\n",
       "         [ 789.0000,  438.0000]],\n",
       "\n",
       "        [[ 140.0000,  237.0000],\n",
       "         [ 174.0000,  221.0000],\n",
       "         [ 169.0000,  250.0000],\n",
       "         [ 177.0000,  267.0000]],\n",
       "\n",
       "        [[ 321.0000,  360.0000],\n",
       "         [ 420.0000,  346.0000],\n",
       "         [ 315.0000,  410.0000],\n",
       "         [ 346.0000,  485.0000]],\n",
       "\n",
       "        [[ 174.0000,  200.0000],\n",
       "         [ 260.0000,  210.0000],\n",
       "         [ 172.0000,  261.0000],\n",
       "         [ 192.0000,  318.0000]],\n",
       "\n",
       "        [[ 158.0000,  183.0000],\n",
       "         [ 236.0000,  182.0000],\n",
       "         [ 189.0000,  249.0000],\n",
       "         [ 197.0000,  286.0000]],\n",
       "\n",
       "        [[ 193.0000,  246.0000],\n",
       "         [ 215.0000,  246.0000],\n",
       "         [ 198.0000,  260.0000],\n",
       "         [ 203.0000,  274.0000]],\n",
       "\n",
       "        [[ 966.0000,  635.0000],\n",
       "         [1119.0000,  655.0000],\n",
       "         [1030.0000,  723.0000],\n",
       "         [1026.0000,  791.0000]],\n",
       "\n",
       "        [[ 202.3810,   77.9902],\n",
       "         [ 220.6599,   75.7405],\n",
       "         [ 214.7544,   90.2700],\n",
       "         [ 216.8898,   93.8845]],\n",
       "\n",
       "        [[2139.0000, 1222.0000],\n",
       "         [2992.0000, 1337.0000],\n",
       "         [2273.0000, 1999.0000],\n",
       "         [2398.0000, 2392.0000]],\n",
       "\n",
       "        [[ 268.0000,  304.0000],\n",
       "         [ 310.0000,  298.0000],\n",
       "         [ 278.0000,  331.0000],\n",
       "         [ 288.0000,  353.0000]],\n",
       "\n",
       "        [[ 162.0000,  183.0000],\n",
       "         [ 260.0000,  176.0000],\n",
       "         [ 243.0000,  237.0000],\n",
       "         [ 224.0000,  282.0000]],\n",
       "\n",
       "        [[ 161.0000,  102.0000],\n",
       "         [ 236.0000,   87.0000],\n",
       "         [ 203.0000,  136.0000],\n",
       "         [ 209.0000,  167.0000]],\n",
       "\n",
       "        [[ 330.0000,  400.0000],\n",
       "         [ 554.0000,  393.0000],\n",
       "         [ 448.0000,  579.0000],\n",
       "         [ 468.0000,  665.0000]],\n",
       "\n",
       "        [[  56.0000,  359.0000],\n",
       "         [  97.0000,  355.0000],\n",
       "         [  69.0000,  382.0000],\n",
       "         [  83.0000,  398.0000]],\n",
       "\n",
       "        [[1316.0000,  424.0000],\n",
       "         [1570.0000,  281.0000],\n",
       "         [1502.0000,  581.0000],\n",
       "         [1582.0000,  664.0000]],\n",
       "\n",
       "        [[ 811.0000, 1118.0000],\n",
       "         [1268.0000, 1118.0000],\n",
       "         [1079.0000, 1320.0000],\n",
       "         [1102.0000, 1547.0000]],\n",
       "\n",
       "        [[ 131.0000,  116.0000],\n",
       "         [ 167.0000,  127.0000],\n",
       "         [ 148.0000,  150.0000],\n",
       "         [ 141.0000,  166.0000]],\n",
       "\n",
       "        [[ 113.0000,  168.0000],\n",
       "         [ 191.0000,  152.0000],\n",
       "         [ 160.0000,  205.0000],\n",
       "         [ 170.0000,  243.0000]],\n",
       "\n",
       "        [[ 986.0000, 1410.0000],\n",
       "         [1456.0000, 1366.0000],\n",
       "         [1240.0000, 1548.0000],\n",
       "         [1268.0000, 1775.0000]],\n",
       "\n",
       "        [[ 378.4899,  350.4002],\n",
       "         [ 453.0354,  375.9690],\n",
       "         [ 380.0000,  405.0000],\n",
       "         [ 372.0000,  447.0000]],\n",
       "\n",
       "        [[ 158.0000,  117.0000],\n",
       "         [ 205.0000,  115.0000],\n",
       "         [ 176.0000,  139.0000],\n",
       "         [ 180.0000,  165.0000]],\n",
       "\n",
       "        [[ 163.0000,  295.0000],\n",
       "         [ 243.0000,  303.0000],\n",
       "         [ 183.0000,  357.0000],\n",
       "         [ 207.0000,  396.0000]],\n",
       "\n",
       "        [[ 233.5553,  134.9560],\n",
       "         [ 257.8416,  131.0295],\n",
       "         [ 238.4998,  149.9349],\n",
       "         [ 244.5848,  163.5477]],\n",
       "\n",
       "        [[ 109.0000,   89.0000],\n",
       "         [ 130.4837,   87.5516],\n",
       "         [ 121.2973,  100.9561],\n",
       "         [ 122.0000,  108.0000]],\n",
       "\n",
       "        [[ 639.0000,  227.0000],\n",
       "         [ 666.0000,  227.0000],\n",
       "         [ 644.0000,  241.0000],\n",
       "         [ 644.0000,  263.0000]],\n",
       "\n",
       "        [[ 654.0000,  538.0000],\n",
       "         [ 920.0000,  595.0000],\n",
       "         [ 721.0000,  758.0000],\n",
       "         [ 717.0000,  864.0000]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//OMVICECAVE/nas/ml_data/AFLW//annotated_imgs/image00144.jpg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(dataset_path + \"/annotated_imgs/\", annotations[\"file_id\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# image = plt.imread(img_name)\\n# landmarks = annotations[annotations[\\'file_id\\'] == img].values[0][1:-2].astype(\\'float\\').reshape(-1, 2)\\n# plt.imshow(image)\\n# plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker=\\'.\\', c=\\'r\\')\\n# plt.savefig(dataset_path + \"/annotaded_imgs/\" + img)\\n# plt.close()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.mkdir(dataset_path + \"/annotaded_imgs/\")\n",
    "import shutil\n",
    "\n",
    "# for dir in [\"0\", \"2\", \"3\"]:\n",
    "#     for img in os.listdir(os.path.join(dataset_path, dir)):\n",
    "#         if img in annotations['file_id'].values:\n",
    "#             img_name = os.path.join(dataset_path, dir, img)\n",
    "\"\"\"\n",
    "# image = plt.imread(img_name)\n",
    "# landmarks = annotations[annotations['file_id'] == img].values[0][1:-2].astype('float').reshape(-1, 2)\n",
    "# plt.imshow(image)\n",
    "# plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
    "# plt.savefig(dataset_path + \"/annotaded_imgs/\" + img)\n",
    "# plt.close()\n",
    "\"\"\"\n",
    "            # shutil.copy(img_name, dataset_path + \"/annotaded_imgs/\" + img)\n",
    "    # print(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14280"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(dataset_path + \"/annotaded_imgs/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14287,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[\"file_id\"].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16418,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[\"face_id\"].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FaceKeypointDataset(annotations, dataset_path + \"/annotaded_imgs/\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "for images, landmarks in dataloader:\n",
    "    print(images.shape)\n",
    "    print(landmarks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs shape: torch.Size([32, 8])\n",
      "labels shape: torch.Size([32, 8])\n",
      "outputs shape: torch.Size([32, 8])\n",
      "labels shape: torch.Size([32, 8])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     11\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\GruppProjekt\\face-keypoint-detection\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\GruppProjekt\\face-keypoint-detection\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\GruppProjekt\\face-keypoint-detection\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\GruppProjekt\\face-keypoint-detection\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m, in \u001b[0;36mFaceKeypointDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     19\u001b[0m     img_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_id\u001b[39m\u001b[38;5;124m\"\u001b[39m][idx])\n\u001b[1;32m---> 20\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     23\u001b[0m         image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([image] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\GruppProjekt\\face-keypoint-detection\\.venv\\Lib\\site-packages\\matplotlib\\pyplot.py:2613\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   2609\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimread)\n\u001b[0;32m   2610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimread\u001b[39m(\n\u001b[0;32m   2611\u001b[0m         fname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath \u001b[38;5;241m|\u001b[39m BinaryIO, \u001b[38;5;28mformat\u001b[39m: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2612\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\GruppProjekt\\face-keypoint-detection\\.venv\\Lib\\site-packages\\matplotlib\\image.py:1502\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parse\u001b[38;5;241m.\u001b[39murlparse(fname)\u001b[38;5;241m.\u001b[39mscheme) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;66;03m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1498\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease open the URL for reading and pass the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult to Pillow, e.g. with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1501\u001b[0m         )\n\u001b[1;32m-> 1502\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimg_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[0;32m   1503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[0;32m   1504\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mPngImagePlugin\u001b[38;5;241m.\u001b[39mPngImageFile) \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m             pil_to_array(image))\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\GruppProjekt\\face-keypoint-detection\\.venv\\Lib\\site-packages\\PIL\\Image.py:3469\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3466\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3469\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3470\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        labels = labels.view(labels.size(0), -1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"outputs shape: {outputs.shape}\")\n",
    "        print(f\"labels shape: {labels.shape}\")\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 10}\")\n",
    "            running_loss = 0.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
