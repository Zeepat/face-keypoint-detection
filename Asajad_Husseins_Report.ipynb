{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>1. Introduction</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Project Overview</u>\n",
    "\n",
    "Facial keypoint detection is a computer vision task that involves identifying and locating specific points on a person's face, such as the eyes, nose, and mouth. This project aims to develop a reliable neural network model that can accurately detect these facial keypoints in various images. By utilizing advanced machine learning techniques and data augmentation methods, the project seeks to enhance the precision and consistency of facial feature detection. This is important for applications like facial recognition systems, virtual makeup tools, and interactive technologies that respond to facial expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### <u>Problem Statement</u>\n",
    "\n",
    "While there have been significant advancements in facial keypoint detection, achieving high accuracy and reliability in real-world conditions remains challenging. Current models often struggle with faces showing extreme angles, partial obstructions, or under different lighting conditions. Additionally, the limited availability of diverse annotated data makes it difficult to create models that work well for everyone. This project aims to overcome these issues by developing a top-performing facial keypoint detection model that remains accurate and dependable across a wide range of real-world scenarios.\n",
    "\n",
    "The main goals of this project are:\n",
    "\n",
    "- Model Development: Create and implement a deep learning model, preferably using convolutional neural networks, optimized for precise facial keypoint detection.\n",
    "\n",
    "- Data Augmentation: Apply advanced data augmentation techniques to increase the diversity of the training dataset, helping the model generalize better to new and unseen facial variations.\n",
    "\n",
    "- Performance Optimization: Fine-tune the model’s settings and use regularization methods to prevent overfitting, ensuring the model performs well on different datasets.\n",
    "\n",
    "- Evaluation and Validation: Thoroughly assess the model’s performance using both numerical metrics and visual inspections, comparing it with existing benchmarks to demonstrate its effectiveness.\n",
    "\n",
    "- Application Integration: Explore how the developed model can be used in areas like facial recognition, emotion detection, and augmented reality, showcasing its practical usefulness and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>2. Handling of Training and Inference</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### <u>Data Preparation:</u>\n",
    "\n",
    "The project utilizes the Facial Keypoints Detection dataset, which contains images of human faces along with annotated keypoints such as the eyes, nose, and mouth. The dataset is sourced from AFLW and comprises approximately 25000 images. Each image is labeled with 20 keypoints, providing precise locations for facial features. The dataset includes diverse facial expressions, lighting conditions, and angles to ensure the model learns to generalize well across different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### <u>Preprocessing Steps:</u>\n",
    "Before training, the data undergoes several preprocessing steps to ensure consistency and improve model performance:\n",
    "\n",
    "1. Data Cleaning: Images with missing or corrupted keypoints are removed to maintain data integrity.\n",
    "\n",
    "2. Normalization: Pixel values of the images are scaled to a range of [0, 1] to facilitate faster and more stable training. Similarly, keypoint coordinates are normalized relative to the image dimensions.\n",
    "\n",
    "3. Data Augmentation: To increase the diversity of the training data, techniques such as rotations, horizontal flips, and brightness adjustments are applied using the Albumentations library. This helps the model become more robust to variations in real-world data.\n",
    "\n",
    "It might be worth mentioning that in training, I (Asajad), used data augmentation but other group mates did not. Results will be explained later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Design Choices:</u>\n",
    "\n",
    "The model is built using a Convolutional Neural Network (CNN) architecture, which is well-suited for image-based tasks. The network consists of multiple convolutional layers followed by pooling layers to extract hierarchical features from the input images. After the convolutional blocks, fully connected layers are used to map the extracted features to the final keypoint coordinates. Dropout layers are included to prevent overfitting by randomly disabling neurons during training.\n",
    "\n",
    "A CNN was chosen for its proven effectiveness in image recognition and localization tasks. The architecture balances depth and complexity to capture intricate facial features without being overly computationally intensive. Utilizing dropout layers enhances the model's ability to generalize by reducing reliance on specific neurons, thereby improving performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Network Architecture](model/network_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Hyperparameters:</u>\n",
    "- Learning Rate: Set to 0.0001, it determines the step size during weight updates. A lower learning rate ensures stable convergence.\n",
    "\n",
    "- Batch Size: Set to 32, it defines the number of samples processed before the model's internal parameters are updated.\n",
    "\n",
    "- Epochs: The model is trained for 50 epochs, allowing sufficient iterations for learning.\n",
    "\n",
    "- Weight Decay: Applied at 1e-5 to prevent overfitting by penalizing large weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Training Pipeline:</u>\n",
    "\n",
    "1. Forward Pass: Input images are fed through the CNN to obtain predicted keypoints.\n",
    "\n",
    "2. Loss Calculation: The Mean Squared Error (MSE) loss function measures the difference between predicted and actual keypoints.\n",
    "\n",
    "3. Backward Pass: Gradients are computed using backpropagation.\n",
    "\n",
    "4. Optimization: The Adam optimizer updates the model's weights based on the gradients.\n",
    "\n",
    "5. Learning Rate Scheduling: The ReduceLROnPlateau scheduler adjusts the learning rate if the validation loss does not improve, aiding in finer convergence.\n",
    "\n",
    "6. Regularization: Dropout layers within the network help prevent overfitting by randomly deactivating neurons during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Techniques Applied:</u>\n",
    "The Albumentations library is employed to apply the following augmentation methods:\n",
    "\n",
    "- Horizontal Flips: Randomly flips images horizontally to simulate different face orientations.\n",
    "\n",
    "- Rotations: Rotates images within a range of ±15 degrees to mimic varied head tilts.\n",
    "\n",
    "- Shift, Scale, and Rotate (ShiftScaleRotate): Combines shifting, scaling, and rotating to create more diverse training samples.\n",
    "\n",
    "- Brightness and Contrast Adjustments: Alters the brightness and contrast to account for different lighting conditions.\n",
    "\n",
    "- Gaussian Blur: Applies blurring to simulate out-of-focus scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results yielded from the data augmentation were insignificant. The others in my group had their lowest validation loss at 0.0048 while mine yielded a validation loss of 0.0047."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Steps Involved:</u>\n",
    "\n",
    "The inference process follows a streamlined sequence to generate keypoint predictions from input images:\n",
    "\n",
    "1. Image Input: A new facial image is provided to the trained model.\n",
    "\n",
    "2. Preprocessing: The image undergoes the same normalization as during training to ensure consistency.\n",
    "\n",
    "3. Model Prediction: The CNN processes the image and outputs predicted keypoint coordinates.\n",
    "\n",
    "4. Post-processing: The normalized keypoints are scaled back to the original image dimensions to obtain accurate positions.\n",
    "\n",
    "5. Visualization: Predicted keypoints are plotted on the image for easy interpretation.\n",
    "\n",
    "\n",
    "The post-processing steps involved:\n",
    "- Denormalization: The predicted keypoint coordinates, initially normalized, are scaled back to match the original image size. This involves multiplying the normalized values by the image's width and height.\n",
    "\n",
    "- Clipping: Ensures that keypoints lie within the image boundaries by clipping any coordinates that fall outside the valid range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
